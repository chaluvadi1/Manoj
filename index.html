<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <title>Manoj</title>
</head>
<body>
<nav class="navbar navbar-dark bg-dark">
    <a class="navbar-brand" href="#">Manoj Chaluvadi</a>
</nav>
<div class="p-4 p-md-5 mb-4 text-dark bg-light">
    <div class="col-md-6 px-0">
        <h1 class="display-4 font-italic">About Me</h1>
        <p class="lead my-3"></p>
    </div>
</div>
<div class="container">
    <p>
        •	10 years of experience in IT industry, related to various aspects involving Data, like providing data, analyzing data, transforming, using ETL tools like Informatica Power Center (7.x, 8.x, 9.x and 10.x), IBM Infosphere Datastage (11.5).</br>
        •	10 years of Data warehousing experience using ETL for data manipulation and Code migration.</br>
        •	Creation of Perl, shell script where it is necessary in aiding ETL jobs and for capturing metadata from Informatica jobs.</br>
        •	Experience in Installations of Informatica Power Center versions 7.x, 8.x on Linux platform. Created groups roles, privileges and assigned them to each user group.</br>
        •	Extensively worked on UNIX shell scripts on automation (Disk space utilization, Informatica Server monitoring and UNIX file system maintenance and cleanup, and scripts using Informatica Command line utilities).</br>
        •	Experience in all phases of Software development life cycle (SDLC) – Project Analysis, Requirements, Design Documentation, Development, Unit Testing, User Acceptance Testing, Implementation, Post Implementation Support and Maintenance.</br>
        •	Documenting standard ETL Naming standards & BEST Practices throughout the ETL process (Transformations, sessions, maps, workflow names, log files, bad files, input, variable, output ports), for different clients.</br>
        •	Creating Technical Design documents.</br>
        •	Worked with different Informatica performance tuning issues with source, target, mapping, transformation, session optimization issues and fine-tuned Transformations to make them more efficient in terms of session performance.</br>
        •	Database experience using Teradata 12/13, Oracle 9i/8.x/7.x, MS SQL Server 2000 and MS Access. Also used SQL Editors such as SQL PLUS, SQL Assistant and TOAD.</br>
        •	Knowledge on open source ETL Tools like PENTAHO AND TALEND.</br>
        •	Knowledge on Hadoop architecture and tools for HDFS like Map-Reduce, PIG, HIVE.</br>
        •	Experience in loading data from HDFS files in PIG and dumping the stored data with desired attributes.</br>
        •	Creating proof of concept with HDFS files to see its feasibility in the client location.</br>
        •	Understanding of object-oriented programming languages like C#, Python, Java and non-object oriented like C.</br>
        •	Created an utility in shell that converts BAI files to regular flat files that replaced proprietary  tool that costs 100k a year for licensing.</br>
    </p>
</div>
<div class="p-4 p-md-5 mb-4 text-dark bg-light">
    <div class="col-md-6 px-0">
        <h1 class="display-4 font-italic">Nationwide</h1>
        <p class="lead my-3">Data Engineer - Columbus Ohio</p>
        <p>Oct-2013 -</br>
             Nationwide Insurance and its affiliated companies are a large group of insurance and financial services companies, over the years at Nationwide I worked at different business areas (Marketing, Insurance, Financial and Actuarial). I worked on a wide range of technologies/tools that included AWS, Docker, Hadoop, Hive, Sqoop, Spark, Datastage, Informatica, Teradata, Netezza, MS SQL Server, Oracle, PostgreSQL, Python, Perl, Ruby, Git, Sub version, Hudson, Jenkins, Kubernetes.
        </p>
    </div>
</div>
<div class="container">
    <p class="lead my-3">Continuous Improvement</p>
    <p>
        •	Migrated Jenkins On Prem Instance to Kubernetes Cluster on AWS cloud using helm. </br>
        •	Architected and automated profile creation for analysis using python.</br>
        •	Creating process that converts unstructured/semi-structured data to structured data.</br>
        •	Creating automated scripts that backup informatica code in xml.</br>
        •	Improve the performance of the current audit process by leveraging oracle sqlloader and temporary audit tables.</br>
        •	Created new type of json parameters avoiding multi table parameters.</br>
        •	Automated creation informatica source/target objects based on source to target documents.</br>
        •	Created a new process that pushes batch process status notifications to a rocket chat channel.</br>
        •	Automated creation of issues in Jira using git and python using REST Api.</br>
        •	Working on a proof of concept that replaces the mapping documents that uses a web interface and leverages json files for metadata and git for change management.</br>
        •	Upgrading Jenkins instance to eliminate critical vulnerabilities found from twistlock.</br>
        •	Upgraded our SharePoint documentation to git wiki pages.</br>
        •	Moved our email statuses to GitHub pages.</br>
    </p>
    <p class="lead my-3">Roles/Responsibilities</p>
    <p>
        •	Understanding the IDQ mappings used for profiling and convert them to informatica power center code.</br>
        •	Writing unit-tests using Ruby rspec against the power center mappings.</br>
        •	Documenting the results of proof of concepts and determine all the pros and cons.</br>
        •	Develop Datastage ETL jobs based on the STM documents created by the analysts using ATDD.</br>
        •	Creating acceptance and unit tests for the tables and jobs.</br>
        •	POC on MapReduce to replace few ETL jobs.</br>
        •	Designing spark jobs using PySpark.</br>
        •	Ensuring the Jenkins build is green for all the jobs.</br>
        •	Resolving testing and production defects.</br>
        •	Helping with release activities, resolving any issues encountered during release activities.</br>
        •	Analyzing alpha/beta loads to determine issues with jobs or processes and fixing them if necessary.</br>
        •	Creating apps and jobs in ESP scheduler.</br>
        •	Running multiple apps in different environment using ESP to get necessary data for alpha and ST testing.</br>
        •	Creating git tags for releases.</br>
        •	Maintaining kubernetes resources within our BSA.</br>
        •	Testing Kerberos environment on sandbox with current development jobs.</br>
        •	Changing Dev environment so that the Kerberos enabled server can be accessed by other utilities.</br>
        •	Helping with release related activities.</br>
        •	Creating new types of parameters called in json format so more information can be stored without using normalized tables.</br>
        •	Rewriting the audit process that utilizes multiple tables to store file/table metadata.</br>
        •	Migrating existing datastage code to informatica.</br>
        •	Rewriting the supporting Perl code so that it can be used with Informatica as it is on a Linux file system instead of HDFS.</br>
        •	Migrating Perl using Urban code deploy.</br>
        •	Developing ETL mappings from the given requirements and unit testing them accordingly.</br>
        •	Using ATDD with ruby rspec for unit testing and cucumber for acceptance testing.</br>
        •	Helping new pair partners understand the team dev/test norms and getting them up to speed.</br>
        •	Converting business requirements into low level design document.</br>
        •	Performance tuning mappings that take longer time to run.</br>
        •	Used Informatica partitioning to improve the performance wherever needed, sometimes got the jobs to complete in 10 minutes from 3 hours.</br>
        •	Making use of flat files to reduce usage on Teradata.</br>
        •	Come up with unique design solutions that have good performance and better readability.</br>
        •	Using pushdown optimization whenever possible, sometimes changed Informatica mappings so that the optimizer would produce even better queries.</br>
        •	Data loads for System Testing and User Acceptance Testing in lower environments.</br>
        •	Created Perl scripts, BTEQ’s and Stored procedures to load history data without creating additional Informatica mappings or new ESP jobs.</br>
        •	Coming up with forklift strategy from lower environments to higher environments.</br>
        •	Supporting the application for any failures or enhancements in production.</br>
        •	Use of Java transformation to replicate the data from input stream.</br>
        •	Analyzing data to determine if the defects that were raised were appropriate as per the design.</br>
        •	Resolving defects and adding appropriate documentation.</br>
        •	Setting up dependencies between jobs in ESP scheduler.</br>
        •	Creating run book so the support team would have knowledge on running the application manually and understanding how to restart the application correctly.</br>
        •	Creating Perl scripts and Perl modules so they could be used to support the application.</br>
        •	Migrating Informatica, Perl, job metadata among different environments.</br>
        •	Converted VSAM input files to ASCII output files so it could be used in our master file creation process which helped eliminating lookups/Joins on Teradata tables.</br>
        •	Used all the different Informatica flat file partitioning techniques, Round robin, Hash User Key, Hash Auto Key, Pass through.</br>
        •	Correcting incorrectly partitioned mappings.</br>
        •	Setting up profiles in Linux for new teammates so they could catchup with the file system easily.</br>
        •	Creating branches in subversion during a release so the working code base would not affect the prod code base.</br>
        •	Organizing show and tell for business users and retro for team mates.</br>
        •	Resolving daily build issues.</br>
    </p>
</div>
<!-- Optional JavaScript; choose one of the two! -->

<!-- Option 1: Bootstrap Bundle with Popper -->
<script src="js/bootstrap.min.js" ></script>

<!-- Option 2: Separate Popper and Bootstrap JS -->

<script src="js/popper.min.js"></script>
<script src="js/jquery-3.5.1.slim.min.js"></script>

</body>
</html><!-- Image and text -->
